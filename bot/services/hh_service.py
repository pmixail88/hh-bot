import aiohttp
import asyncio
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
import logging
from urllib.parse import urlencode
import re

from core.config import HHConfig
from services.cache import CacheService

logger = logging.getLogger(__name__)

class HHService:
    def __init__(self, config: HHConfig):
        self.config = config
        self.base_url = "https://api.hh.ru"
        self.cache = CacheService()
        self.timeout = aiohttp.ClientTimeout(total=config.timeout)
        self.areas_cache = {}
        self.semaphore = asyncio.Semaphore(5)
        
        # –ö—ç—à –¥–ª—è ID —Ä–µ–≥–∏–æ–Ω–æ–≤
        #self.areas_cache = {}

    async def search_vacancies(self, search_filter: Any) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        cache_key = f"vacancies:{self._get_filter_hash(search_filter)}"
        
        # –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 30 –º–∏–Ω—É—Ç –≤–º–µ—Å—Ç–æ 10 and –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
        cached = await self.cache.get(cache_key)
        if cached:
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(cached)} –≤–∞–∫–∞–Ω—Å–∏–π –≤ –∫—ç—à–µ")
            return cached
        
        
        try:
            params = await self._build_search_params(search_filter)
            vacancies = []
            
            logger.info(f"üîç –ü–æ–∏—Å–∫ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {params}")
            
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                #data = await self._fetch_page(session, params, 0)
                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                async with self.semaphore:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
                    data = await self._fetch_page(session, params, 0)
                    
                if not data:
                    logger.info("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç HH API")
                    return []
                
                items = data.get('items', [])
                logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(items)} –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                vacancies.extend(items)
                
                total_pages = data.get('pages', 1)
                found = data.get('found', 0)
                logger.info(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {found} –≤–∞–∫–∞–Ω—Å–∏–π, —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                pages_to_fetch = min(total_pages, 10)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
                
                # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                tasks = [
                    self._fetch_page(session, params, page) 
                    for page in range(1, pages_to_fetch)
                ]
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, dict):
                        page_items = result.get('items', [])
                        vacancies.extend(page_items)
                        logger.info(f"üìÑ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(page_items)} –≤–∞–∫–∞–Ω—Å–∏–π —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                vacancies = vacancies[:self.config.max_results]
                logger.info(f"üì¶ –ò—Ç–æ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
                
                # –ü–∞—Ä—Å–∏–º –≤–∞–∫–∞–Ω—Å–∏–∏
                parsed_vacancies = []
                parse_errors = 0
                
                for vacancy_data in vacancies:
                    parsed = self._parse_vacancy(vacancy_data)
                    if parsed:
                        parsed_vacancies.append(parsed)
                    else:
                        parse_errors += 1
                
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(parsed_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π, –æ—à–∏–±–æ–∫: {parse_errors}")
                
                # –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 15 –º–∏–Ω—É—Ç
                #await self.cache.set(cache_key, parsed_vacancies, expire=600) - 10 –º–∏–Ω—É—Ç
                if parsed_vacancies:
                    await self.cache.set(cache_key, parsed_vacancies, expire=1200)
                return parsed_vacancies
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π: {e}")
            return []

    async def _build_search_params(self, search_filter: Any) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è HH API"""
        params = {
            'per_page': 100,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            'page': 0,
            'order_by': 'publication_time',  # –°–Ω–∞—á–∞–ª–∞ –Ω–æ–≤—ã–µ
            'search_field': 'name'  # –ò—Å–∫–∞—Ç—å –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        if search_filter.keywords and search_filter.keywords.strip():
            params['text'] = search_filter.keywords.strip()
        else:
            # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ —É–∫–∞–∑–∞–Ω—ã, –∏—â–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
            params['text'] = "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç"
        
        # –†–µ–≥–∏–æ–Ω
        if search_filter.region and search_filter.region.strip():
            area_id = await self._get_area_id(search_filter.region.strip())
            if area_id:
                params['area'] = area_id
            else:
                # –ï—Å–ª–∏ —Ä–µ–≥–∏–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –†–æ—Å—Å–∏—é
                params['area'] = 113  # –†–æ—Å—Å–∏—è
        else:
            params['area'] = 113  # –†–æ—Å—Å–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ó–∞—Ä–ø–ª–∞—Ç–∞
        if search_filter.salary_from:
            params['salary'] = search_filter.salary_from
            params['only_with_salary'] = True
        
        # –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã
        if search_filter.experience:
            experience_map = {
                '–Ω–µ—Ç –æ–ø—ã—Ç–∞': 'noExperience',
                '–æ—Ç 1 –≥–æ–¥–∞': 'between1And3', 
                '–æ—Ç 3 –ª–µ—Ç': 'between3And6',
                '–æ—Ç 6 –ª–µ—Ç': 'moreThan6'
            }
            params['experience'] = experience_map.get(
                search_filter.experience.lower().strip(), 
                search_filter.experience
            )
        
        # –¢–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏
        if search_filter.employment:
            employment_map = {
                '–ø–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å': 'full',
                '—á–∞—Å—Ç–∏—á–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å': 'part',
                '–ø—Ä–æ–µ–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞': 'project',
                '–≤–æ–ª–æ–Ω—Ç–µ—Ä—Å—Ç–≤–æ': 'volunteer',
                '—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞': 'probation'
            }
            params['employment'] = employment_map.get(
                search_filter.employment.lower().strip(),
                search_filter.employment
            )
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã
        if search_filter.schedule:
            schedule_map = {
                '–ø–æ–ª–Ω—ã–π –¥–µ–Ω—å': 'fullDay',
                '—Å–º–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫': 'shift',
                '–≥–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫': 'flexible',
                '—É–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞': 'remote',
                '–≤–∞—Ö—Ç–æ–≤—ã–π –º–µ—Ç–æ–¥': 'flyInFlyOut'
            }
            params['schedule'] = schedule_map.get(
                search_filter.schedule.lower().strip(),
                search_filter.schedule
            )
        
        # –ü–µ—Ä–∏–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 –¥–µ–Ω—å)
        params['period'] = search_filter.period or 1
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        clean_params = {k: v for k, v in params.items() if v is not None and v != ''}
        logger.info(f"üîÑ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞: {clean_params}")
        
        return clean_params

    async def _fetch_page(self, session: aiohttp.ClientSession, params: Dict, page: int) -> Optional[Dict]:
        """–ó–∞–ø—Ä–æ—Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏"""
        try:
            params['page'] = page
            # –û—á–∏—â–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç None –∑–Ω–∞—á–µ–Ω–∏–π
            clean_params = {k: v for k, v in params.items() if v is not None}
            
            async with session.get(f"{self.base_url}/vacancies", params=clean_params) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    logger.debug(f"‚ùå –û—à–∏–±–∫–∞ HH API: {response.status} –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {clean_params}")
                    return None
        except asyncio.TimeoutError:
            logger.debug("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –≤–∞–∫–∞–Ω—Å–∏–π")
            return None
        except Exception as e:
            logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –≤–∞–∫–∞–Ω—Å–∏–π: {e}")
            return None

    async def _get_area_id(self, region_name: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å ID —Ä–µ–≥–∏–æ–Ω–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if region_name in self.areas_cache:
            return self.areas_cache[region_name]
        
        try:
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(
                    f"{self.base_url}/suggests/areas",
                    params={'text': region_name}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data.get('items'):
                            area_id = data['items'][0]['id']
                            self.areas_cache[region_name] = area_id
                            return area_id
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ä–µ–≥–∏–æ–Ω–∞ {region_name}: {e}")
        
        return None

    def _parse_vacancy(self, raw_vacancy: Dict) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ HH API"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ raw_vacancy –Ω–µ None –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è
            if not raw_vacancy or 'id' not in raw_vacancy:
                logger.debug("‚ùå –ü—É—Å—Ç–∞—è –≤–∞–∫–∞–Ω—Å–∏—è –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ID")
                return None
            
            salary = raw_vacancy.get('salary', {}) or {}
            employer = raw_vacancy.get('employer', {}) or {}
            area = raw_vacancy.get('area', {}) or {}
            snippet = raw_vacancy.get('snippet', {}) or {}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            if not employer.get('name') or not raw_vacancy.get('name'):
                logger.debug("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–∏")
                return None
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_at = None
            if raw_vacancy.get('published_at'):
                try:
                    published_str = raw_vacancy['published_at'].replace('Z', '+00:00')
                    #published_at = datetime.fromisoformat(published_str)
                    published_at = datetime.utcnow()
                except ValueError:
                    published_at = datetime.utcnow()
            else:
                published_at = datetime.utcnow()
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
            requirement = snippet.get('requirement', '') or ''
            responsibility = snippet.get('responsibility', '') or ''

            description_parts = []
            if requirement:
                # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                requirement = re.sub('<[^<]+?>', '', requirement)  # ‚úÖ re —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω
                description_parts.append(f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: {requirement}")
            if responsibility:
                responsibility = re.sub('<[^<]+?>', '', responsibility)  # ‚úÖ re —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω
                description_parts.append(f"–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏: {responsibility}")

            description = ' '.join(description_parts)
            
            if len(description) > 2000:
                description = description[:2000] + "..."
            elif not description:
                description = "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ"
            
            parsed = {
                'hh_id': raw_vacancy['id'],
                'name': raw_vacancy.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è').strip(),
                'company_name': employer.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–æ').strip(),
                'area_name': area.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω'),
                'salary_from': salary.get('from'),
                'salary_to': salary.get('to'),
                'salary_currency': salary.get('currency'),
                'salary_gross': salary.get('gross'),
                'experience': raw_vacancy.get('experience', {}).get('name'),
                'schedule': raw_vacancy.get('schedule', {}).get('name'),
                'employment': raw_vacancy.get('employment', {}).get('name'),
                'description': description,
                'skills': '',
                'url': raw_vacancy.get('alternate_url', ''),
                'published_at': published_at,
                #'employer_id': employer.get('id')
            }
            
            logger.debug(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è: {parsed['name']}")
            return parsed
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ {raw_vacancy.get('id', 'unknown')}: {e}")
            return None

    def _get_filter_hash(self, search_filter: Any) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞"""
        params = [
            search_filter.keywords or '',
            search_filter.region or '',
            str(search_filter.salary_from or ''),
            str(search_filter.salary_to or ''),
            search_filter.experience or '',
            search_filter.employment or '',
            search_filter.schedule or '',
            str(search_filter.period or '')
        ]
        return str(hash(''.join(params)))

    async def check_vacancy_archived(self, vacancy_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –≤–∞–∫–∞–Ω—Å–∏—è"""
        try:
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(f"{self.base_url}/vacancies/{vacancy_id}") as response:
                    return response.status == 404  # –ï—Å–ª–∏ 404 - –≤–∞–∫–∞–Ω—Å–∏—è –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∞
        except Exception:
            return True

    async def test_connection(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ HH API"""
        try:
            async with aiohttp.ClientSession(timeout=10) as session:
                async with session.get(f"{self.base_url}/vacancies", params={'text': 'test'}) as response:
                    return response.status == 200
        except Exception:
            return False